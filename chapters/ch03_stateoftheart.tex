%!TEX root = ../report.tex
\documentclass[../report.tex]{subfiles}
\begin{document}
    \chapter{State of the Art}

    \section{Explainability Vs Interpretability Methods}
    Use as many sections as you need in your related work to group content into logical groups
    \section{Taxonomy of Explainability Methods}

    \section{Explainability Methods for Neural Networks}
    \subsection{Types}
    \subsection{Choice of Methods for Further Evaluation}
    \subsubsection{Occlusion Sensitivity Maps}
    Occlusion sensitivity Mapping is a model-agnostic perturbation based method, which generates explanations by manipulating parts of the input image. The approach is computationally expensive, O(\#simultaneous occlusions * \#features * \#ablations\_per\_eval * 1/\#strides), and is included in this work to verify if Gestalt Matcher model is focusing on key facial features, or simply using the surrounding context to produce predictions. This is achieved by systematically occluding different portions of the input image with a black square or rectangular mask, and computing the difference in outputs (logit scores of the target class). In this work, we use a  black square mask of dimensions 10x10. Important portions of the input when occluded, result in relatively larger logit score differences, than the trivial ones. The differences are plotted on the image, yielding the occlusion sensitivity maps.
    
    \subsubsection{Deconvolution}
    Zeiler and Fergus proposed the \enquote{Deconvolution} approach to visualize and provide insights into the functions learned by intermediate layers of a CNN. It is one of the earliest attribution techniques, which produces visualizations based on computing gradient of loss function with respect to a given input. The work acts a baseline till date for development and evaluation of new pixel attribution techniques.\\
    The method uses a deconvolution counterpart for every building block of a CNN, to obtain reverse mapping from features to input pixels. The idea of deconvolution was first introduced by Zeiler et al. \cite{zeiler2011adaptive}, as a way to perform unsupervised learning. In order to obtain attribution maps using the Deconvolution approach, the first step is to attach each block of convnet with its deconvolution counterpart as shown in the figure 1. Every activation except the ones belonging to the class of interest is set to zero. The activation value is then backpropagated through the deconvolution blocks such as unpooling, rectification and transposed convolution, all the way to the input layer. Deconvolution blocks act according to a pre-defined set of rules. The transposed convolution block performs the inverse of convolution operation by using transposed versions of the same filters. This is equivalent to flipping a given filter both in vertical and horizontal directions. In order to backtrack activations through max-pooling layer (.i.e. using the unpooling layer), indices corresponding to maximum activations in every layer, are first stored during the forward pass and later retrieved during the back propagation phase. However, the use of indices or switches from the forward pass, constrains the visualization on the input image \cite {guided_backprop}.\\ 
    Authors test their method on an AlexNet \cite{} trained on the ImageNet \cite{krizhevsky2012imagenet}, Caltech-101 \cite{} and Caltech-256 \cite{} and PASCAL2012 \cite{} datasets. As a first step, they visualize the top 9 feature maps of the each of the first five layers, to show the proportional increase of complexity in features with respect to their receptive fields. The visualizations are obtained by backtracking the strongest activation of a feature map for most of the data samples, all the way until a given input, using the deconvolution rules.The paper also discusses about the proportionality between the time taken for a given layer to learn features and its corresponding depth. Further, it shows that features learned by top layers are more invariant to transformations like translations, rotations and scale changes.\\
    The work evaluates itself by qualitatively comparing its resulting attribution maps with occlusion sensitivity maps. Occlusion sensitivity maps (\cite perturbation method) are obtained by systematically occluding portion of an image and analyzing the given classifier’s output, to determine the most discriminative regions as shown in the first image of Figure \ref{osm}.
    \begin{figure}[ht!]
    \includegraphics[width=\textwidth]{images/occlusion_sensitivity_map}
    \caption{An illustration of occlusion sensitivity mapping}
    \label{osm}
\end{figure}
    
    
    
 \subsubsection{Saliency Maps}
 Simonyan et al. propose two visualization techniques with intents to generate an image which maximizes the class score, and to compute a class-specific saliency map for a given input. The first technique numerically optimizes the input image while the other computes the spatial support of a given class in an input. This work is one of the earliest to leverage saliency maps for the task of weakly-supervised object segmentation. Authors demonstrate the proposed techniques by applying to a deep convnet trained on the ILSVRC-2013 dataset. \cite{ILSVRC15}
 
 \subsubsection*{Class Model Visualization}
 The intention of this technique is to numerically generate an image which is representative of the target category with respect to the convolutional net’s class scoring model. This is achieved by finding a L2-regularised image such that the logit $S_{c}$  of a given class c is maximized:
 
 %\begin{equation*}
 	
 %\end{equation*}
 
 where  $\lambda$ refers to the regularization parameter and I is a local optimum, which can be found with help of back propagation. The optimization process uses the mean image of the data set as the initial value. The work also mentions about the prominence of visualizations produced by using logit scores over soft-max/unnormalized class scores.   
    
 \subsubsection*{Image-Specific Class Saliency}
  The objective of this technique is to rank pixels of an input image, based on their impact on class scores ($S_c$). Authors provide a couple of interpretations for the class score values/ logits with respect to which saliency maps are created:
  \begin{enumerate}
  	\item A linear approximation of the function learned by  neural network in a local neighbourhood of the input image.
  	\item Higher the saliency associated with a pixel, lesser it needs to be altered in order to increase it respective class’s score.  
  \end{enumerate}

 The derivative of class score with respect to input image is found using back propagation as described by the equation below:
 
  %\begin{equation*}
 
 %\end{equation*}
 
 In order to obtain the saliency map for a multi-channel image, the maximum magnitude of gradient for a given position across channels is used. Class saliency maps thus produced are used as initial points  to compute object segmentation mask using the GraphCut algorithm. (\ref Graphcut). Foreground and background portions are considered as Gaussian Mixture Models and the former is estimated from pixels with saliency value higher than the 95\% quantile of the image’s saliency distribution.On the other hand, the latter is estimated from pixels with saliency smaller than 30\% quantile.\\
 The work evaluates its outputs on test split of the data set for the localization task in the ILSVRC-2013 \cite{} challenge, where it achieves 46.4\% top-5 error in spite of its simplicity.
 In hindsight, apart from the strategy used to reverse the ReLU layer this approach is equivalent to Deconvnet.
 
 \subsubsection{Guided Backpropagation}
 Springenberg et al. proposed a new variant of the deconvolution approach in their work, as a means to analyze their \enquote{All Convolutional Net} architecture, which replaces max-pooling layers by convolutional layers with increased stride. The first objective of this work was to empirically prove the equivalence (in terms of predictive performance) between a max-pooling layer and a convolutional layer with an increased stride. This was achieved by evaluating a custom cnn model with max-pooling layers against its convolutional counterpart on three datasets: CIFAR-10, CIFAR-100 and ILSVRC 2012. In all cases, performances of the all convolutional models were on par with their max pooling counterparts. The second objective was to determine the quality of representations learned by the intermediate layers of the all convolutional neural network models. In order to achieve this, authors proposed a visualization  approach, which can be considered as a slight modification of the Deconvnet \cite{} technique.
 \subsubsection*{Back propagation through ReLU}
 One of the most significant difference between Saliency Maps \cite{}, Deconvnet \cite{} and Guided backprop \cite{} approaches is the strategy used by these methods to backpropagate gradients through the ReLU layer. 
 
 \begin{itemize}
 	\item Saliency maps approach backpropagates gradients of positions with respective to non-negative activations.
 	%\begin{equation*}
 	%\end{equation*}
 	\item On the other hand, the deconvnet approach allows only positive gradients to flow in reverse direction.
 	%\begin{equation*}
 	%\end{equation*}
 	\item The guided-backprop approach combines the above mentioned methods and masks out values for which at least  one of activation or gradient values is negative. This is performed with an intention to avoid the reverse of negative gradients of neurons which reduce the activation of the target neuronal unit.
	%\begin{equation*}
	%\end{equation*}
 \end{itemize}

Figure b illustrates differences between back propagation strategies with help of an example feature map. 

The term \enquote{guided backpropagation} comes from the use of the additional navigation signal, to selectively back propagate only the positive gradients of the positively activated neuronal units. Though guided backprop was devised to show the learning capability of the all convolutional network architectures, authors show the effectiveness of the technique on the ones with max-pooling units. Guided backprop produced significantly more accurate representation, especially  for higher layers, when compared to Deconvnet and Saliency maps.

\subsubsection{Deep LIFT}
 Shrikumar et al. propose an attribution technique, which assigns scores to input pixels based on their contribution to change in activation of each neuronal unit with respect to a reference value, which is chosen based on the problem in hand. Authors claim the technique to be computationally inexpensive, and yield meaningful representations in comparison with other methods. Besides, the technique is devised in a way that it is suitable for neural net variants apart from CNN like Recurrent Neural Networks (RNN).\\
 Intuitively, Deep LIFT seeks to explain the deviation in output from some reference output in terms of deviation in input from its respective reference. The motivation to use a reference value, arises from the need to handle the \enquote{saturation problem}.
 \subsubsection*{Saturation problem}
 The saturation problem can be explained intuitively with an example. The figure illustrates a simple neural network whose outputs saturates, when the sum of its inputs ($i_1$ and $i_2$) exceeds 1. Application of any perturbation or gradient based attribution method, to this scenario would lead to creation of undesirable artifacts. For the methods of earlier type, perturbing inputs will not cause any changes in the output. On the other hand, gradient based methods will suffer from lack of gradients in the region where $i_1+i_2 > 1$.\\
 \subsubsection*{Working}
 Deep LIFT handles this problem by using a reference value, enabling it to backpropagate the importance signal even in zero and discontinuous gradient situations.\\
 The contributions computed by DeepLIFT is analogous to the idea of partial derivatives, except for the fact that change in input is computed with respect to a finite value (activation difference) in the earlier, in contrast to an infinitesimal value in the latter. The concept of \enquote{multipliers} is used to achieve the same.
 %\begin{equation*}
 %\end{equation*}
 $x$ is the input neuron with a difference from reference $\Delta x$, and t is the target neuron with a difference from reference $\Delta t$. C is then the contribution of $\Delta x$ to $\Delta t$.
 Multipliers obey chain rule
  %\begin{equation*}
 %\end{equation*}
 Along with the custom chain rule, the paper also defines a set of rules for neurons of every kind of neural network layer, to assign contribution scores their inputs:
 \begin{itemize}
 	\item Linear rule for linear layers such as the fully-connected and convolutions
 	\item Rescale rule for non-linear transformations such as ReLU, tanh or sigmoid
 	\item Reveal-cancel rule which enables the measurement of marginal effect of having an input over all possible orderings, similar to Shapely values \cite{}. 
 \end{itemize}
The technique also takes in to account that a zero contribution could be due to cancellation of positive and negative contributions from a pair of entities. The application of DeepLIFT also depends on the choice of target neuron’s layer, logit or softmax layer in a classifier network, for example. Authors demonstrate the approach by applying it to a CNN trained on MNIST dataset\cite{} and an RNN trained on simulated genomic data.\\
The benchmarking on MNIST classifier for various methods was performed on basis of the change in log-odds score, when a selected pixels of a given image belonging to class c0 were erased to convert it to an image of some target class ct. The upper section of figure illustrates the result of masking pixels ordered as the most significant for converting to target classes 3 and 6. As graphically represented in bottom part of figure, DeepLIFT outperformed other backpropagation based methods.

\subsubsection{GradCAM}
Selvaraju et al. propose GradCAM, a pixel-attribution technique leveraging the gradients of a given target class with respect to a convolution layer. This approach can be considered as a generalization of CAM for CNNs with fully connected layers. Besides, the technique is applicable to neural network architectures for tasks such as image captioning and visual-question answering. The work also comes up with a means to convert class-agnostic fine grained visualization approaches like Guided-Backprop and Deconvolution to be class-discriminative in nature.\\
Grad-CAM is one of the most widely used approaches in obtaining attribution map based explanations, for neural network models used with medical data (provide references). Concisely put, Grad-CAM creates a visualization of which parts of an input image a convolutional layer “looks” for a certain class prediction. The working of Grad-CAM can be described in the following steps:
\begin{enumerate}
	\item Perform a forward pass of the input image through CNN to obtain logit scores for all classes.
	\item  Except for the logit activation of the class of interest, set other activations to zero.
	\item Backpropagate the gradient of the class of interest all the way to the chosen convolutional layer containing k feature maps Ak. Del yc /delAk
	\item \textbf{Global pooling:} For every feature map, weigh their pixels based on the gradient value, and obtain their weighted mean value scaled by constant Z
	  %\begin{equation*}
	  %\end{equation*}
	where $y_c$ refers to logit score for class c, $A^k$ refers to the $k^{th}$ activation map of dimensions ixj and $\alpha$ refers to the computed weightage.
	\item Obtain the $\alpha$ weighted average of all pooled feature maps and apply a ReLu to filter out negative values, if any present.
	%begin{equation*}
	%end{equation*}
	where Lc refers to the localization map produced by the Grad-CAM for the class of interest c and Ak refers to the kth feature map.
	\item An element-wise multiplication operation is applied on the scaled version (input image dimensions) of Grad-CAM’s maps, and outputs of fine-grained visualization approaches like Guided-Backpropagation and Deconvolution, to obtain fine-grained yet class discriminative visualizations. These combinations are termed as Guided-GradCAM and Guided-Deconvolution approaches respectively.
	\item Guided Grad-CAM visualizations that are both high-resolution and class-discriminative”
	\item Typically, last convolutional layers of a neural network model are chosen for Grad-CAM as they contain high-level semantics and detailed spatial information (\ref same paper). This is because the attribution maps produced by the methods becomes progressively qualitatively worse as we use earlier convolutional layers which have relatively lesser receptive fields.
	\item In a classification network, logit scores of the target class are used for gradient computations. However, any differentiable activation value can be backpropagated. The embedding based Grad-CAM discussed in the section, draws motivation from this fact. 
	\item In the original implementation of the attribution method, a Rectified Linear Unit (ReLU) function is applied on heat maps, to obtain only regions that positively affect the given prediction. However, to get better insights into prediction decisions, this work uses unfiltered heat maps, which consists of negatively correlated regions as well.  
	\item The work is evaluated from the perspective of three different tasks: weakly-supervised localization, weakly-supervised segmentation and Pointing Game experiment (\cite Deep features for discriminative localization). The approach is also evaluated and bench marked on its ability to generate discriminative, trust-worthy, faithful and interpretable attribution maps. A couple of neural network models with different architectures (AlexNet and VGG-16) are used for evaluation, to determine the method’s performance consistency across architectures.
	\item Finally, the work also demonstrates the association of a given concept with a neuron, similar to the ones presented by (Visualizing and understanding convolutional networks, Object detectors emerge in deep scene CNNs.)
	\item Authors also demonstrate it use in analyzing failure modes in neural network models and identifying bias in dataset.
	\item The approach is considered to be computationally in-expensive when compared to perturbation based methods such as LIME or SHAP, yet producing interpretable and discriminative attribution maps.
\end{enumerate}
\end{document}
